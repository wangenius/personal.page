---
title: Neural Networks
sidebar_position: 1
slug: neural-networks
---

Artificial Neural Networks (ANNs) surged in popularity during the 1980s, faded in the 1990s, and then returned with the rise of deep learning. Inspired by the human brain, a neural network comprises layers of interconnected neurons that apply linear transformations followed by nonlinear activations.

Milestones such as the backpropagation (BP) algorithm enabled efficient gradient-based training. Experiments in vision and speech recognition demonstrated that neural networks excel at extracting hierarchical features: early layers detect local patterns (edges, phonemes), while deeper layers combine them into higher-level concepts (objects, words).

Modern architectures—CNNs, RNNs/LSTMs, Transformers, Graph Neural Networks—specialize for different data modalities but share the same core idea: learnable linear transformations plus nonlinearities optimized end-to-end.
