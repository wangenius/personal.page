---
title: Support Vector Machines
sidebar_position: 4
slug: svm
---

Support Vector Machines (SVMs) construct a maximum-margin hyperplane that separates classes. Only a subset of training points—**support vectors**—define the boundary.

Core ideas:

- **Hard vs. Soft Margin** – Soft margins allow misclassifications controlled by the `C` penalty.
- **Kernels** – Map data into high-dimensional spaces without explicit feature expansion (RBF, polynomial, sigmoid, etc.).
- **Dual Formulation** – Enables efficient training via quadratic programming.

SVMs work well on medium-sized datasets with clear margins and remain competitive for text classification and bioinformatics tasks.
