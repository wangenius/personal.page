---
title: Decision Trees
sidebar_position: 3
slug: decision-trees
---

Decision trees split the feature space into regions using hierarchical if–else rules. Each internal node chooses a feature and threshold; each leaf stores a prediction.

Key ingredients:

- **Split Criteria** – Information gain (entropy), Gini impurity, variance reduction.
- **Stopping Rules** – Minimum samples per leaf, maximum depth, or early stopping via validation.
- **Pruning** – Removes branches that overfit (cost-complexity pruning).

Tree ensembles such as Random Forests and Gradient Boosted Trees (XGBoost, LightGBM, CatBoost) deliver state-of-the-art results on many tabular datasets.
