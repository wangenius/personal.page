---
title: ASCII and Unicode
sidebar_position: 3
slug: ascii-unicode
---

1. **ASCII** – 8-bit encoding with 7 effective bits; covers basic Latin characters and control codes.
2. **Unicode** – Unifies all scripts under a single code space, eliminating mojibake caused by incompatible encodings.
3. **UTF-8** – Variable-length encoding of Unicode that remains backward-compatible with ASCII and minimizes storage for Latin text.

```c
#include <stdio.h>
int main()
{
    printf("hello, world\n");
    return 0;
}
```

Source files such as `hello.c` are stored as sequences of bytes. Each byte is an integer corresponding to a character—`35` for `#`, `105` for `i`, and so on. Lines end with the newline character `\n` (value `10`). Files that contain only ASCII characters are called *text files*; everything else is treated as binary.

This illustrates a fundamental idea: every piece of information in a computer—disk files, instructions, user data, network packets—is simply a string of bits. Context determines whether a byte sequence represents an integer, a floating-point number, a string, or machine code. Because machine representations are finite approximations of mathematical values, programmers must understand their limitations.

C and Unix grew up together. The Unix kernel, tools, and libraries were written in C, making the OS highly portable. C’s concise design (documented in 261 pages of *The C Programming Language*) made it easy to learn and implement on new hardware. Although pointers and the lack of high-level abstractions can be challenging, C remains the language of choice for systems programming, while higher-level languages like C++ and Java address application-level needs.
