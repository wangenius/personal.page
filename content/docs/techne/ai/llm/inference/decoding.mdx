---
title: 解码策略 (Decoding Strategies)
---

LLM 的训练目标是预测下一个 Token 的概率分布 $P(w_t | w_{1:t-1})$。但在推理（Inference）阶段，如何从这个概率分布中选择具体的 Token，决定了生成文本的质量、多样性和创造力。这个选择过程就是**解码策略**。

## 确定性解码 (Deterministic Methods)

这种策略在相同输入下，总是产生相同的输出。

### 1. Greedy Search (贪婪搜索)
- **原理**: 每一步都选择概率最高的那个 Token。
- **优点**: 速度快，计算简单。
- **缺点**: 容易陷入局部最优，生成内容枯燥、重复，且容易出现"死循环"。它看不到全局最优解。

### 2. Beam Search (束搜索)
- **原理**: 每一步保留概率最高的 $k$ 个候选序列（Beam Width）。到最后选择总概率最高的那一条路径。
- **优点**: 比贪婪搜索产生的句子更通顺、逻辑更连贯。
- **缺点**: 计算开销大。生成的文本通常比较平庸、保守，缺乏惊喜感（对于翻译任务很好，但对于创意写作可能太死板）。

## 随机性解码 (Stochastic Methods)

引入随机性（Sampling），让模型有机会选择非最大概率的词，从而增加多样性。

### 1. Temperature (温度)
- **原理**: 在 Softmax 之前调整 Logits。
  - $T < 1$（低温）: 放大高概率词的优势，分布变得尖锐，输出更保守、确定。
  - $T > 1$（高温）: 缩小概率差异，分布变得平坦，低概率词被选中的机会增加，输出更发散、有创意，但也更容易胡言乱语。

### 2. Top-k Sampling
- **原理**: 仅在概率最高的 $k$ 个 Token 中进行采样，将其他 Token 的概率置零。
- **作用**: 切断了那些极低概率（可能是错误或无关）的词的尾部，保证了生成内容的相关性。
- **局限**: $k$ 是固定的。有时合理的词很少（$k$ 太大），有时合理的词很多（$k$ 太小），不够灵活。

### 3. Top-p (Nucleus) Sampling
- **原理**: 动态选择候选词。选择概率之和达到阈值 $p$（如 0.9）的最小集合，在这个集合中采样。
- **优势**: 自适应。当模型确定时，候选集很小；当模型不确定时，候选集变大。这是目前最主流的采样策略。

## 惩罚机制 (Penalties)

### Repetition Penalty (重复惩罚)
为了缓解 LLM 复读机的毛病，对已经生成过的 Token 的 Logits 施加惩罚，降低其再次被选中的概率。

## 总结

- **翻译/摘要/代码**: 通常偏向确定性策略（Beam Search）或低温度，追求准确。
- **聊天/创作/头脑风暴**: 通常使用 Top-p + 适当的温度（0.7-0.9），追求多样性和拟人化。
