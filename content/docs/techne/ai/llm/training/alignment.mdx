---
title: 对齐 (Alignment)
---

预训练得到的 Base Model 虽然博学，但它只是一个“文本续写机”。为了让它成为一个有用、无害、诚实（Helpful, Harmless, Honest - 3H 原则）的助手，我们需要进行对齐训练。

对齐通常分为两个主要阶段：

## 1. SFT (Supervised Fine-Tuning, 监督微调)

这是将 Base Model 转化为 Chat Model 的第一步。

- **数据形式**: 高质量的 `(Prompt, Response)` 对。
- **过程**: 依然是 Next Token Prediction，但数据不再是杂乱的网页文本，而是人工精选的指令和完美的回答。
- **目的**: 规范模型的输出格式，学会“指令遵循（Instruction Following）”。
- **结果**: 模型学会了对话模式，知道当用户提问时，应该回答而不是续写问题。

## 2. 偏好对齐 (Preference Alignment)

SFT 只能让模型模仿人类的回答，但很难让模型学会判断“哪个回答更好”。偏好对齐引入了人类（或强模型）的价值观判断。

### RLHF (Reinforcement Learning from Human Feedback)

这是 ChatGPT 成功的关键技术，通常包含三个步骤：

1.  **SFT**: 训练一个基础的对话模型。
2.  **Reward Modeling (RM)**: 训练一个奖励模型。让 SFT 模型对同一个问题生成多个回答，人类标注员对这些回答进行排名（Ranking）。RM 学习预测人类的偏好分数。
3.  **PPO (Proximal Policy Optimization)**: 使用强化学习算法，利用 RM 的打分来优化 SFT 模型的参数，使其生成的回答能获得更高的奖励。

### DPO (Direct Preference Optimization)

DPO 是 2023 年提出的新算法，它跳过了复杂的 Reward Model 和 PPO 阶段。

- **原理**: 直接在偏好数据对 `(Winning Response, Losing Response)` 上优化模型。
- **优势**: 训练更稳定，计算资源消耗更少，效果通常不输 RLHF。
- **现状**: DPO 及其变体（IPO, KTO）已成为开源社区对齐模型的主流方法。

## 对齐税 (Alignment Tax)

过度的对齐可能会导致模型在某些客观能力（如创造力、数学推理）上的下降，这种现象被称为“对齐税”。如何在安全性与能力之间寻找平衡，是当前研究的热点。
