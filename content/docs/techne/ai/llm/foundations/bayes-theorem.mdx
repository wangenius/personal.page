---
title: 贝叶斯定理
---

## Bayes' Theorem

贝叶斯定理回答的是一个极其实际的问题：
在拿到一条新证据之后，某个假设为真的概率应该如何合理更新？

对应的数学形式是：

$$
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}
$$

一句话可以记成：
“后验 = 似然 × 先验 / 归一化因子”

---

## 拆解（务实）

- $P(A)$ 先验（Prior）：在看到证据 B 之前，你对 A 的主观信念。

- $P(B \mid A)$ 似然（Likelihood）：在 A 为真的前提下，你观察到 B 的可能性有多大？

- $P(B)$ 证据概率（Marginal likelihood）：所有可能导致 B 的情形的总概率（用于归一化）。

- $P(A \mid B)$ 后验（Posterior）：在看到 B 之后，你对 A 的信念被更新到的新状态。

---

## 最具代表性的现实例子（直观）

### 医疗检测

假设某种疾病在总体人群中的发病率非常低（先验很小），
即便某个检测工具的准确率很高，它依然会产生一定比例的假阳性。

在这种情况下，贝叶斯视角给出的结论是：
一次“阳性”结果，并不意味着你大概率已经患病，因为先验本身就极小。

这个结论与多数人的直觉相反，但却是现代循证医学与很多 AI 系统的底层逻辑。

---

## 创业者视角的理解（更前瞻）

放到创业语境里，贝叶斯思维可以理解为：
把每一次新信息都当作一条证据，用来持续更新自己的世界模型。

- 新市场数据（B）出现 → 更新你对“需求是否真实”（A）的信念；
- 新客户反馈（B）到来 → 修正你对“产品是否有价值”（A）的判断；
- 投资人说“不”（B） → 并不自动意味着 A 为假，只能说明 (P(B mid A)) 里存在噪声。

本质上，它要求我们：
不要把任何既有认知当作“定论”，而是随着证据流不断动态更新。

---

## 更务实的创业公式版（你可能更爱）

如果用贝叶斯的方式判断一个需求是否值得做，可以写成：

$$
P(\text{需求真实} \mid \text{用户反馈})
= \frac{P(\text{反馈} \mid \text{需求真实}) \cdot P(\text{需求真实})}
{P(\text{反馈})}
$$

从这个式子能直接读出几条对创业非常实际的启发：

- 如果你一开始就认为需求存在的概率很低（先验小）；
- 仅有一两个用户说“想要”，对整体判断的影响有限（信号噪声比不高）；
- 你需要的是更多、更高质量的证据来逐步更新先验。

这其实和你常说的一句话高度一致：
不是“需求驱动”一切，而是“生产力 / 产品力”在很多时候更关键——因为先验（你的世界观和能力边界）往往比零散证据更重要。

下面按你关心的几个方向，一层一层往下展开：

> Transformer 作为图模型 → Attention 与条件独立 → 世界模型 → RLHF = 改先验 / 重加权 → 变分推断 & GPT → 能不能做因果推断

---

## 一、Transformer ≈ 一种特殊的贝叶斯图模型

先把语言模型写成标准概率形式：

$$
P(x_1,\dots,x_T)=\prod_{t=1}^{T}P(x_t\mid x_{<t})
$$

这是一个链式图模型（每个 token 依赖所有过去 tokens）。

Transformer 做的事情：
把每个条件分布 $P(x_t\mid x_{<t})$ 用一个深度网络去近似，
这个网络内部可以被理解成一个有向图 + 多层消息传递：

- 节点：隐状态 $h_t^(l)$（第 l 层，第 t 个位置）
- 有向边：
  - 层内：从 $h_1^(l-1)$ 到 $h_t^(l-1)$ 指向 $h_t^(l)$ 的边（自注意力）
  - 层间：$h_t^(l-1)$ 指向 $h_t^(l)$ 的边（前馈网络）

一个很粗暴的图形理解（只画一个位置）：

```
Layer L:   h_t^(L)
             ^
             |
Layer 2:   h_t^(2)  <--- (attend to h_1..h_t^(1))
             ^
             |
Layer 1:   h_t^(1)  <--- (attend to h_1..h_t^(0))
             ^
             |
Input:     x_t (embedding)
```

整张图展开是一个巨大 DAG（有向无环图）：

- 所有 $h_t^(l)$ 是随机变量（在贝叶斯视角下都可看作 latent）
- 参数 $\theta$ 决定了每条边的条件分布 $P(h_t^(l) \mid \text{parents})$

从图模型角度看：

- 训练时：在这个 DAG 上调整参数，让
  $$
  P_\theta(x_t\mid x_{<t})=\text{softmax}(W h_t^(L))
  $$
  尽量贴近真实数据。
- 推理时：用当前上下文 $(x_{\le t})$ 做证据，前向传递，把所有隐变量的“分布”压缩成一个点估计（即网络输出的向量）。

这就是“图模型 + 贝叶斯更新被网络近似实现了”。

---

## 二、Attention 与条件独立（CI）

传统 n-gram 或 RNN 的隐式假设是：

- 最近的 token 比远处重要
- 依赖结构是线性的（Markov 性比较强）

而 Self-Attention 在图模型中的角色是：

> 在所有可能的父节点中，按权重选择“谁对我最重要”。

### 1️⃣ Attention 的公式本质上是一个归一化权重

$$
\alpha_{t\leftarrow s} = \text{softmax}_s \big(Q_t K_s^\top \big)
$$

- 你可以把 $\alpha\_{t\leftarrow s}$ 当成“在位置 t 时，认为 s 是原因/证据的权重”
- 然后：
  $$
  h_t^(l) = \sum_s \alpha_{t\leftarrow s} V_s
  $$

这和图模型里的消息传递（message passing / belief propagation）非常像：
每个目标节点从所有父节点收消息，然后按权重整合。

### 2️⃣ Attention 其实隐含着“稀疏依赖 / 条件独立”

- 多头注意力：每个 head 学一套“依赖模式”，相当于多种图结构的混合
- Masked Self-Attention：强行规定只能看过去，保证有向性（避免信息泄露）

如果从 CI 的视角看：

- 对于某个 head，如果一个位置 s 的注意力权重长期接近 0，可以视为“在该 head 作用下，t 与 s 条件独立（给定其它 tokens）”
- 模型通过学习注意力矩阵，等价于在学习“这句话中，谁和谁是重要的条件依赖”。

简化理解一句话：

> Attention = 自动学习条件依赖结构的机制，训练时不断逼近一个更合理的贝叶斯网络。

---

## 三、大模型为什么自然长出“世界模型”

世界模型 = 能在内部压缩出“世界的隐变量结构”，
比如：物理常识、社会规则、角色关系、时间顺序、动机等。

在自回归 LM 里，你只优化一个目标：

$$
\max_\theta \mathbb{E}_{x\sim\text{语料}} \sum_t \log P_\theta(x_t\mid x_{<t})
$$

但要把这件事做好，你几乎被迫学到：

1. 哪些概念经常一起出现 → 语义结构
2. 哪些事件有时间先后或因果顺序 → 简单因果/脚本结构
3. 哪些叙事/推理模式在各种文本里反复出现 → “思维模板”

在贝叶斯语言里：

- 模型参数 $\theta$ 就是你对“世界怎么生成这些文本”的后验信念；
- 训练结束得到的 $\theta^\*$ 是在所有训练数据条件下的 MAP 估计；
- 推理阶段，每次看到新上下文，相当于在这个后验之上做局部更新。

所以你看到的“世界模型”行为（例如合理猜测未明说的信息），
是因为：

> 模型已经学到一个能解释大量文本的生成分布。
> 在这个分布里，很多“世界结构”是高概率路径，自然被利用出来。

---

## 四、RLHF：本质是对原始后验做再加权 / 改先验

预训练 LM 给你的是一个分布 $P_0(y\mid x)$。
RLHF / DPO / RLAIF 等做的事，其数学形态大致可以写成：

$$
P_{\text{new}}(y\mid x)\propto P_0(y\mid x)\cdot \exp(\beta r(x,y))
$$

- $P_0$：预训练得到的“知识后验”
- $r(x,y)$：人类偏好模型 / 奖励模型
- $\beta$：温度 / 强度超参

这等价于：

> 在原有的“世界分布”基础上，再乘上一个偏好因子，然后归一化。

从贝叶斯角度，你可以理解为：

- 原先有一个后验 $P(\theta\mid \text{预训练数据})$
- RLHF 引入了一个新的“伪似然”或“伪先验”：喜欢的输出得更高权重
- 最终得到的是“更符合人类偏好的后验”

关键点：

- RLHF 不是主要用来“灌知识”，而是用来“调行为风格”
- 知识主要来自预训练语料，RLHF 是重排输出的秩 / 重塑决策边界

所以常见现象：

- RLHF 强的模型：更安全、更礼貌、更重视对话体验
- 但如果极端，会损失一点“原始野性推理能力”——因为某些“太生猛”的输出被大量压制了。

---

## 五、变分推断（VI）与 GPT 的关系

严格的贝叶斯世界里，我们想要：

$$
P(\theta\mid \mathcal{D}) \propto P(\mathcal{D}\mid \theta)P(\theta)
$$

但真实深度网络太大，这个后验算不出来，于是有两种常见近似：

1. MAP 近似（最大后验）：
   - 就是现在主流训练方式：找一个最优参数点 $\theta^\*$, 代表整个后验。
   - 对应梯度下降 + 正则化。

2. 变分推断（VI）：
   - 给参数或隐变量假设一个近似分布 $q_\phi(\theta)$
   - 优化 KL 距离：$\min_\phi mathrm{KL}\big(q_\phi(\theta) Vert P(\theta \mid \mathcal{D})big)$

GPT 的标准训练更接近于 MAP；
但你可以从 VI 的视角去看它的行为模式：

- 把巨大网络看成“一个具有隐变量的生成模型”，
  隐变量就是所有层的 $h_t^(l)$。

- 前向传播时，网络其实做了一次“近似后验推断”：

  $$
  q(h \mid x) = \delta(h - f_\theta(x))
  $$

  ——也就是“把后验分布收缩成一个点估计”（delta 分布）。

- 如果未来我们对参数引入显式的分布（Bayesian Neural Net）、
  或者在大模型上做 Dropout / Ensemble，其实都可以视作在做某种 VI。

所以一句话总结：

> 今天的 GPT：用 MAP 估计 + 点后验推断来近似贝叶斯世界。
> 真正的全贝叶斯版大模型在理论上更干净，但算力太贵。

---

## 六、语言模型到底能不能做因果推断？

这是一个很容易被误解的点，需要分清三种概率：

1. 观察分布（观测）：

   $$
   P(Y\mid X=x)
   $$

   语言模型非常擅长，因为整个训练数据都是观察数据。

2. 干预分布（do-operator）：

   $$
   P(Y\mid \text{do}(X=x))
   $$

   这需要明确的因果图 + 结构方程。
   仅靠文本统计做到这一步，严格意义上是不够的，除非：
   - 数据中隐含了大量实验/对照、政策变化、自然实验；
   - 模型间接学到了这些结构。

3. 反事实分布（counterfactual）：
   $$
   P(Y_{X \leftarrow x'}\mid X=x, Y=y)
   $$
   这个更难，要有个“世界生成机”，能在某个节点强制改值并重算路径。

语言模型现在的能力：

- _弱因果_：可以通过模式识别、常识、经验，给出非常像因果解释的回答；
- _强因果_：如果你要严肃到“识别混杂变量、建立结构方程、反事实推断”，
  需要在 LM 之上再构建一个显式的因果图 / 程序化推理层。

你可以这样看：

> 预训练 LM 给你的是一个超强的“因果假设生成器 + 经验库”；
> 真正严肃的因果推断需要你额外显式建模和约束。

---

## 最后帮你收个口：如何在脑子里统一这些东西？

可以用一个稍微抽象、但非常好用的统一视角来记：

1. 世界在按自己的规律运行 → 产生各种数据（包括文本）
2. 我们假设存在一个生成模型（图结构 + 参数），在背后“生成”这些数据
3. 通过贝叶斯更新：用观察到的数据不断修正我们对这个生成模型的信念（后验）
4. Transformer：用巨大的神经网络来近似这个生成模型以及更新过程
5. 预训练：让模型去拟合“世界是如何产出文本”的概率分布
6. RLHF：在原有“知识后验”上再乘上一个“人类偏好因子”，然后重新归一化
7. 推理：每次看到一个新的上下文，相当于在这个后验上做一次局部更新，并输出下一步的“信念”

如果你想继续往下延展，有两个自然的方向：

- 画一个“Transformer 作为贝叶斯图模型”的简化结构图（节点、边和信息流一目了然，方便讲给别人听）；
- 或者，直接结合一个具体问题，比如“预测经济衰退 / 疾病诊断 / 用户留存”，设计一套明确利用贝叶斯思想 + LLM 的决策流程。

你可以直接说：要“图”，还是要“决策流程实战”，我就往那边继续挖。
