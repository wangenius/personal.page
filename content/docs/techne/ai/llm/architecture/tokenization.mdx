---
title: Tokenization (分词)
---

**Tokenization（分词）** 是 LLM 处理文本的第一步。它将人类可读的文本字符串转换为模型可处理的数字序列（Token IDs）。

## 什么是 Token？

Token 是文本的最小语义单位。它不完全等同于单词（Word）或字符（Character）。

- **Word**: "apple"
- **Subword**: "ing" in "playing"
- **Character**: "a"

对于英文，一个 Token 大约等于 0.75 个单词。对于中文，一个 Token 通常对应 1-2 个汉字。

## 常见的分词算法

### BPE (Byte Pair Encoding)

这是目前最主流的分词算法（GPT-2, GPT-3, LLaMA 均使用）。

1.  **初始化**: 将词表初始化为所有单个字符。
2.  **统计**: 统计语料库中相邻字符对的出现频率。
3.  **合并**: 将频率最高的字符对合并为一个新的 Token。
4.  **循环**: 重复步骤 2-3，直到词表大小达到预设值（如 32k, 50k, 100k）。

### Byte-level BPE

为了处理 Unicode 字符（如 Emoji、中文、特殊符号），现代 LLM 通常在字节级别（Byte-level）而不是字符级别进行 BPE。这意味着模型可以直接处理任何 UTF-8 编码的文本，而不会遇到 `<UNK>`（未知字符）问题。

## 词表大小 (Vocabulary Size)

- **GPT-4 / LLaMA 3**: ~100k - 128k
- **LLaMA 2**: 32k

词表越大，单个 Token 能表达的语义越丰富，压缩率越高（同样的文本序列长度更短），但也会增加 Embedding 层和输出层的参数量。

## Tokenization 的坑

1.  **数字处理**: 很多分词器将数字切分得很碎（如 "1024" 可能被切分为 "10" 和 "24"），导致 LLM 不擅长算术。
2.  **多语言不平衡**: 英文 Token 压缩率高，小语种（如泰语、印地语）可能一个字就被切成多个 Token，导致处理成本（API 费用）更高且效果更差。
3.  **Glitch Tokens**: 某些罕见的字符串组合（如 "SolidGoldMagikarp"）在训练数据中极少出现但被分为了独立 Token，导致模型遇到它们时会崩溃或输出乱码。
