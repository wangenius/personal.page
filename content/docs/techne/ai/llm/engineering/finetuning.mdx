---
title: 微调 (Fine-tuning)
---

**微调（Fine-tuning）** 是在预训练模型（Base Model）的基础上，使用特定领域的数据进行进一步训练，以提升模型在特定任务上的表现。

## 什么时候需要微调？

Prompt Engineering 就像是给模型“写便条”，而 Fine-tuning 则是给模型“做脑科手术”。
当遇到以下情况时，考虑微调：

1.  **知识注入**: 需要模型掌握公司内部的特定术语或知识（虽然 RAG 通常是更好的选择）。
2.  **格式固化**: 需要模型极其严格地遵循某种复杂的输出格式（如特定的 JSON Schema）。
3.  **风格模仿**: 需要模型模仿特定的语气、说话方式（如模仿鲁迅）。
4.  **降低成本**: 使用微调后的小模型（如 7B）来替代昂贵的大模型（如 GPT-4）。

## PEFT (Parameter-Efficient Fine-Tuning)

全量微调（Full Fine-tuning）需要更新模型的所有参数，成本极高。PEFT 技术允许我们只更新极少量的参数，就能达到接近全量微调的效果。

### LoRA (Low-Rank Adaptation)

LoRA 是目前最流行的微调技术。

- **原理**: 冻结预训练模型的权重，在 Transformer 层旁边增加两个低秩矩阵（Low-rank Matrices）$A$ 和 $B$。训练时只更新 $A$ 和 $B$。
- **优势**: 显存占用极低（可以在单张消费级显卡上微调大模型），训练速度快，生成的权重文件很小（几 MB 到几百 MB）。

### QLoRA

QLoRA 将 LoRA 与量化（Quantization）结合。

- **原理**: 将基座模型量化为 4-bit 加载，然后进行 LoRA 微调。
- **意义**: 彻底降低了微调门槛，使得在单张 24G 显存的 3090/4090 上微调 30B/70B 级别的模型成为可能。

## 微调 vs RAG

| 特性         | RAG (检索增强)       | Fine-tuning (微调)           |
| :----------- | :------------------- | :--------------------------- |
| **知识更新** | 实时，只需更新数据库 | 慢，需要重新训练             |
| **数据隐私** | 数据留在本地         | 数据需上传或在本地训练       |
| **幻觉问题** | 较低 (基于检索)      | 依然存在                     |
| **适用场景** | 事实问答、私有知识库 | 风格模仿、特定格式、特定任务 |

通常建议：**先尝试 Prompt Engineering，再尝试 RAG，最后才考虑 Fine-tuning。** 很多时候，RAG + Few-shot Prompt 就能解决问题。
